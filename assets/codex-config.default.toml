# Default Codex CLI + MCP config.
#
# bootstrap-agentic-coding.sh copies this to `~/.codex/config.toml` if that file
# doesn't exist yet. The placeholder `__FS_ALLOWED_DIR__` is replaced with a real
# path that exists on disk.

# --- Defaults (used when no profile is selected) ---
model = "gpt-5.3-codex"

# You can set this per-profile if you only want "high" reasoning sometimes.
# model_reasoning_effort = "high"

sandbox_mode = "workspace-write"
approval_policy = "on-request"
web_search = "live"

# --- Notifications ---
#
# The notifier prefers Slack Web API (threaded messages), then falls back to an
# incoming webhook, then falls back to a local desktop notification:
#   1) SLACK_BOT_TOKEN + SLACK_CHANNEL (chat.postMessage, threaded by session)
#   2) SLACK_WEBHOOK_URL (Incoming Webhook, unthreaded)
#   3) notify-send (desktop notification)
#
# The bootstrap script installs the notifier to __CODEX_DIR__/bin/codex_notify_slack.py.
notify = ["python3", "__CODEX_DIR__/bin/codex_notify_slack.py"]

[sandbox_workspace_write]
network_access = true

[features]
rmcp_client = true

# --- Profiles (use: `codex --profile <name>`) ---
#
# Note: Profiles override root-level settings. Consider also setting:
#   profile = "<name>"
# at the top level to make one profile the default.

[profiles.read]
model_reasoning_effort = "high"
sandbox_mode = "read-only"
approval_policy = "untrusted"

[profiles.write]
model_reasoning_effort = "high"
sandbox_mode = "workspace-write"
approval_policy = "on-failure"

[profiles.auto]
model_reasoning_effort = "high"
sandbox_mode = "workspace-write"
approval_policy = "on-request"

[profiles.yolo]
model_reasoning_effort = "high"
sandbox_mode = "danger-full-access"
approval_policy = "never"

# --- MCP servers ---
[mcp_servers.filesystem]
command = "npx"
args = ["-y", "@modelcontextprotocol/server-filesystem", "__FS_ALLOWED_DIR__"]

[mcp_servers.serena]
command = "uvx"
args = [
  "--from",
  "git+https://github.com/oraios/serena",
  "serena",
  "start-mcp-server",
  "--context",
  "codex",
]

[mcp_servers.context7]
command = "npx"
args = ["-y", "@upstash/context7-mcp@latest"]

[mcp_servers.github]
command = "docker"
args = ["run","-i","--rm","-e","GITHUB_PERSONAL_ACCESS_TOKEN","ghcr.io/github/github-mcp-server"]
env_vars = ["GITHUB_PERSONAL_ACCESS_TOKEN"]

[mcp_servers.atlassian]
url = "https://mcp.atlassian.com/v1/mcp"

[mcp_servers.playwright]
command = "npx"
args = ["-y", "@playwright/mcp@latest"]

# --- Local LLM server setup ---
[model_providers.local-llama]
name = "local-llama (llama.cpp)"
base_url = "http://127.0.0.1:8001/v1"
wire_api = "responses"

[profiles.minimax]
model = "minimax"                        # must match llama-server --alias
model_provider = "local-llama"
model_reasoning_effort = "xhigh"
model_context_window = 128000
model_auto_compact_token_limit = 118000  # To be tuned
tool_output_token_limit = 12000          # Could be lowered to save context
features.remote_compaction = false       # Disable context compaction using OpenAI model
features.remote_models = false           # Disable OpenAI model
web_search = "disabled"                  # From v0.92 onward

[profiles.qwen3]
model = "qwen3-coder-next"                      # must match llama-server --alias
model_provider = "local-llama"
model_reasoning_effort = "xhigh"
model_context_window = 128000
model_auto_compact_token_limit = 118000  # To be tuned
tool_output_token_limit = 12000          # Could be lowered to save context
features.remote_compaction = false       # Disable context compaction using OpenAI model
features.remote_models = false           # Disable OpenAI model
web_search = "disabled"                  # From v0.92 onward
